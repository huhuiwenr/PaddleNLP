# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-11-18 17:30+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../model_zoo/transformers.rst:2
msgid "PaddleNLP Transformer API"
msgstr ""

#: ../model_zoo/transformers.rst:4
msgid ""
"随着深度学习的发展，NLP领域涌现了一大批高质量的Transformer类预训练模型，多次刷新各种NLP任务SOTA（State of the "
"Art）。 PaddleNLP为用户提供了常用的 "
"``BERT``、``ERNIE``、``ALBERT``、``RoBERTa``、``XLNet`` 等经典结构预训练模型， "
"让开发者能够方便快捷应用各类Transformer预训练模型及其下游任务。"
msgstr ""

#: ../model_zoo/transformers.rst:10
msgid "Transformer预训练模型汇总"
msgstr ""

#: ../model_zoo/transformers.rst:13
msgid ""
"下表汇总了介绍了目前PaddleNLP支持的各类预训练模型以及对应预训练权重。我们目前提供了 **25** 种网络结构， **117** "
"种预训练的参数权重供用户使用， 其中包含了 **57** 种中文语言模型的预训练权重。"
msgstr ""

#: ../model_zoo/transformers.rst:17 ../model_zoo/transformers.rst:538
msgid "Model"
msgstr ""

#: ../model_zoo/transformers.rst:17
msgid "Pretrained Weight"
msgstr ""

#: ../model_zoo/transformers.rst:17
msgid "Language"
msgstr ""

#: ../model_zoo/transformers.rst:17
msgid "Details of the model"
msgstr ""

#: ../model_zoo/transformers.rst:19 ../model_zoo/transformers.rst:540
msgid "ALBERT_"
msgstr ""

#: ../model_zoo/transformers.rst:19
msgid "``albert-base-v1``"
msgstr ""

#: ../model_zoo/transformers.rst:19 ../model_zoo/transformers.rst:23
#: ../model_zoo/transformers.rst:27 ../model_zoo/transformers.rst:31
#: ../model_zoo/transformers.rst:35 ../model_zoo/transformers.rst:39
#: ../model_zoo/transformers.rst:43 ../model_zoo/transformers.rst:47
#: ../model_zoo/transformers.rst:75 ../model_zoo/transformers.rst:79
#: ../model_zoo/transformers.rst:83 ../model_zoo/transformers.rst:87
#: ../model_zoo/transformers.rst:91 ../model_zoo/transformers.rst:95
#: ../model_zoo/transformers.rst:147 ../model_zoo/transformers.rst:186
#: ../model_zoo/transformers.rst:190 ../model_zoo/transformers.rst:194
#: ../model_zoo/transformers.rst:198 ../model_zoo/transformers.rst:202
#: ../model_zoo/transformers.rst:206 ../model_zoo/transformers.rst:210
#: ../model_zoo/transformers.rst:214 ../model_zoo/transformers.rst:218
#: ../model_zoo/transformers.rst:222 ../model_zoo/transformers.rst:226
#: ../model_zoo/transformers.rst:231 ../model_zoo/transformers.rst:236
#: ../model_zoo/transformers.rst:240 ../model_zoo/transformers.rst:244
#: ../model_zoo/transformers.rst:276 ../model_zoo/transformers.rst:280
#: ../model_zoo/transformers.rst:284 ../model_zoo/transformers.rst:292
#: ../model_zoo/transformers.rst:296 ../model_zoo/transformers.rst:300
#: ../model_zoo/transformers.rst:304 ../model_zoo/transformers.rst:322
#: ../model_zoo/transformers.rst:326 ../model_zoo/transformers.rst:330
#: ../model_zoo/transformers.rst:334 ../model_zoo/transformers.rst:338
#: ../model_zoo/transformers.rst:342 ../model_zoo/transformers.rst:346
#: ../model_zoo/transformers.rst:350 ../model_zoo/transformers.rst:358
#: ../model_zoo/transformers.rst:426 ../model_zoo/transformers.rst:430
#: ../model_zoo/transformers.rst:439 ../model_zoo/transformers.rst:444
#: ../model_zoo/transformers.rst:449 ../model_zoo/transformers.rst:453
#: ../model_zoo/transformers.rst:457 ../model_zoo/transformers.rst:461
#: ../model_zoo/transformers.rst:466 ../model_zoo/transformers.rst:471
#: ../model_zoo/transformers.rst:476 ../model_zoo/transformers.rst:503
#: ../model_zoo/transformers.rst:507 ../model_zoo/transformers.rst:511
#: ../model_zoo/transformers.rst:515
msgid "English"
msgstr ""

#: ../model_zoo/transformers.rst:19
msgid ""
"12 repeating layers, 128 embedding, 768-hidden, 12-heads, 11M parameters."
" ALBERT base model"
msgstr ""

#: ../model_zoo/transformers.rst:23
msgid "``albert-large-v1``"
msgstr ""

#: ../model_zoo/transformers.rst:23
msgid ""
"24 repeating layers, 128 embedding, 1024-hidden, 16-heads, 17M "
"parameters. ALBERT large model"
msgstr ""

#: ../model_zoo/transformers.rst:27
msgid "``albert-xlarge-v1``"
msgstr ""

#: ../model_zoo/transformers.rst:27
msgid ""
"24 repeating layers, 128 embedding, 2048-hidden, 16-heads, 58M "
"parameters. ALBERT xlarge model"
msgstr ""

#: ../model_zoo/transformers.rst:31
msgid "``albert-xxlarge-v1``"
msgstr ""

#: ../model_zoo/transformers.rst:31
msgid ""
"12 repeating layers, 128 embedding, 4096-hidden, 64-heads, 223M "
"parameters. ALBERT xxlarge model"
msgstr ""

#: ../model_zoo/transformers.rst:35
msgid "``albert-base-v2``"
msgstr ""

#: ../model_zoo/transformers.rst:35
msgid ""
"12 repeating layers, 128 embedding, 768-hidden, 12-heads, 11M parameters."
" ALBERT base model (version2)"
msgstr ""

#: ../model_zoo/transformers.rst:39
msgid "``albert-large-v2``"
msgstr ""

#: ../model_zoo/transformers.rst:39
msgid ""
"24 repeating layers, 128 embedding, 1024-hidden, 16-heads, 17M "
"parameters. ALBERT large model (version2)"
msgstr ""

#: ../model_zoo/transformers.rst:43
msgid "``albert-xlarge-v2``"
msgstr ""

#: ../model_zoo/transformers.rst:43
msgid ""
"24 repeating layers, 128 embedding, 2048-hidden, 16-heads, 58M "
"parameters. ALBERT xlarge model (version2)"
msgstr ""

#: ../model_zoo/transformers.rst:47
msgid "``albert-xxlarge-v2``"
msgstr ""

#: ../model_zoo/transformers.rst:47
msgid ""
"12 repeating layers, 128 embedding, 4096-hidden, 64-heads, 223M "
"parameters. ALBERT xxlarge model (version2)"
msgstr ""

#: ../model_zoo/transformers.rst:51
msgid "``albert-chinese-tiny``"
msgstr ""

#: ../model_zoo/transformers.rst:51 ../model_zoo/transformers.rst:55
#: ../model_zoo/transformers.rst:59 ../model_zoo/transformers.rst:63
#: ../model_zoo/transformers.rst:67 ../model_zoo/transformers.rst:71
#: ../model_zoo/transformers.rst:111 ../model_zoo/transformers.rst:116
#: ../model_zoo/transformers.rst:122 ../model_zoo/transformers.rst:128
#: ../model_zoo/transformers.rst:132 ../model_zoo/transformers.rst:136
#: ../model_zoo/transformers.rst:153 ../model_zoo/transformers.rst:158
#: ../model_zoo/transformers.rst:163 ../model_zoo/transformers.rst:248
#: ../model_zoo/transformers.rst:252 ../model_zoo/transformers.rst:256
#: ../model_zoo/transformers.rst:260 ../model_zoo/transformers.rst:264
#: ../model_zoo/transformers.rst:268 ../model_zoo/transformers.rst:272
#: ../model_zoo/transformers.rst:288 ../model_zoo/transformers.rst:309
#: ../model_zoo/transformers.rst:313 ../model_zoo/transformers.rst:317
#: ../model_zoo/transformers.rst:354 ../model_zoo/transformers.rst:362
#: ../model_zoo/transformers.rst:366 ../model_zoo/transformers.rst:370
#: ../model_zoo/transformers.rst:374 ../model_zoo/transformers.rst:378
#: ../model_zoo/transformers.rst:383 ../model_zoo/transformers.rst:388
#: ../model_zoo/transformers.rst:391 ../model_zoo/transformers.rst:394
#: ../model_zoo/transformers.rst:398 ../model_zoo/transformers.rst:402
#: ../model_zoo/transformers.rst:406 ../model_zoo/transformers.rst:410
#: ../model_zoo/transformers.rst:414 ../model_zoo/transformers.rst:418
#: ../model_zoo/transformers.rst:422 ../model_zoo/transformers.rst:434
#: ../model_zoo/transformers.rst:481 ../model_zoo/transformers.rst:486
#: ../model_zoo/transformers.rst:491 ../model_zoo/transformers.rst:495
#: ../model_zoo/transformers.rst:499 ../model_zoo/transformers.rst:519
#: ../model_zoo/transformers.rst:523 ../model_zoo/transformers.rst:527
msgid "Chinese"
msgstr ""

#: ../model_zoo/transformers.rst:51
msgid ""
"4 repeating layers, 128 embedding, 312-hidden, 12-heads, 4M parameters. "
"ALBERT tiny model (Chinese)"
msgstr ""

#: ../model_zoo/transformers.rst:55
msgid "``albert-chinese-small``"
msgstr ""

#: ../model_zoo/transformers.rst:55
msgid ""
"6 repeating layers, 128 embedding, 384-hidden, 12-heads, _M parameters. "
"ALBERT small model (Chinese)"
msgstr ""

#: ../model_zoo/transformers.rst:59
msgid "``albert-chinese-base``"
msgstr ""

#: ../model_zoo/transformers.rst:59
msgid ""
"12 repeating layers, 128 embedding, 768-hidden, 12-heads, 12M parameters."
" ALBERT base model (Chinese)"
msgstr ""

#: ../model_zoo/transformers.rst:63
msgid "``albert-chinese-large``"
msgstr ""

#: ../model_zoo/transformers.rst:63
msgid ""
"24 repeating layers, 128 embedding, 1024-hidden, 16-heads, 18M "
"parameters. ALBERT large model (Chinese)"
msgstr ""

#: ../model_zoo/transformers.rst:67
msgid "``albert-chinese-xlarge``"
msgstr ""

#: ../model_zoo/transformers.rst:67
msgid ""
"24 repeating layers, 128 embedding, 2048-hidden, 16-heads, 60M "
"parameters. ALBERT xlarge model (Chinese)"
msgstr ""

#: ../model_zoo/transformers.rst:71
msgid "``albert-chinese-xxlarge``"
msgstr ""

#: ../model_zoo/transformers.rst:71
msgid ""
"12 repeating layers, 128 embedding, 4096-hidden, 16-heads, 235M "
"parameters. ALBERT xxlarge model (Chinese)"
msgstr ""

#: ../model_zoo/transformers.rst:75 ../model_zoo/transformers.rst:542
msgid "BART_"
msgstr ""

#: ../model_zoo/transformers.rst:75
msgid "``bart-base``"
msgstr ""

#: ../model_zoo/transformers.rst:75
msgid "12-layer, 768-hidden, 12-heads, 217M parameters. BART base model (English)"
msgstr ""

#: ../model_zoo/transformers.rst:79
msgid "``bart-large``"
msgstr ""

#: ../model_zoo/transformers.rst:79
msgid ""
"24-layer, 768-hidden, 16-heads, 509M parameters. BART large model "
"(English)."
msgstr ""

#: ../model_zoo/transformers.rst:83 ../model_zoo/transformers.rst:544
msgid "BERT_"
msgstr ""

#: ../model_zoo/transformers.rst:83
msgid "``bert-base-uncased``"
msgstr ""

#: ../model_zoo/transformers.rst:83
msgid ""
"12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers.rst:87
msgid "``bert-large-uncased``"
msgstr ""

#: ../model_zoo/transformers.rst:87 ../model_zoo/transformers.rst:284
#: ../model_zoo/transformers.rst:300
msgid ""
"24-layer, 1024-hidden, 16-heads, 336M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers.rst:91
msgid "``bert-base-cased``"
msgstr ""

#: ../model_zoo/transformers.rst:91
msgid ""
"12-layer, 768-hidden, 12-heads, 109M parameters. Trained on cased English"
" text."
msgstr ""

#: ../model_zoo/transformers.rst:95
msgid "``bert-large-cased``"
msgstr ""

#: ../model_zoo/transformers.rst:95
msgid ""
"24-layer, 1024-hidden, 16-heads, 335M parameters. Trained on cased "
"English text."
msgstr ""

#: ../model_zoo/transformers.rst:99
msgid "``bert-base-multilingual-uncased``"
msgstr ""

#: ../model_zoo/transformers.rst:99 ../model_zoo/transformers.rst:105
#: ../model_zoo/transformers.rst:140
msgid "Multilingual"
msgstr ""

#: ../model_zoo/transformers.rst:99
msgid ""
"12-layer, 768-hidden, 12-heads, 168M parameters. Trained on lower-cased "
"text in the top 102 languages with the largest Wikipedias."
msgstr ""

#: ../model_zoo/transformers.rst:105
msgid "``bert-base-multilingual-cased``"
msgstr ""

#: ../model_zoo/transformers.rst:105
msgid ""
"12-layer, 768-hidden, 12-heads, 179M parameters. Trained on cased text in"
" the top 104 languages with the largest Wikipedias."
msgstr ""

#: ../model_zoo/transformers.rst:111
msgid "``bert-base-chinese``"
msgstr ""

#: ../model_zoo/transformers.rst:111
msgid ""
"12-layer, 768-hidden, 12-heads, 108M parameters. Trained on cased Chinese"
" Simplified and Traditional text."
msgstr ""

#: ../model_zoo/transformers.rst:116
msgid "``bert-wwm-chinese``"
msgstr ""

#: ../model_zoo/transformers.rst:116
msgid ""
"12-layer, 768-hidden, 12-heads, 108M parameters. Trained on cased Chinese"
" Simplified and Traditional text using Whole-Word-Masking."
msgstr ""

#: ../model_zoo/transformers.rst:122
msgid "``bert-wwm-ext-chinese``"
msgstr ""

#: ../model_zoo/transformers.rst:122
msgid ""
"12-layer, 768-hidden, 12-heads, 108M parameters. Trained on cased Chinese"
" Simplified and Traditional text using Whole-Word-Masking with extented "
"data."
msgstr ""

#: ../model_zoo/transformers.rst:128
msgid "``junnyu/ckiplab-bert-base-chinese-ner``"
msgstr ""

#: ../model_zoo/transformers.rst:128
msgid "12-layer, 768-hidden, 12-heads, 102M parameters. Finetuned on NER task."
msgstr ""

#: ../model_zoo/transformers.rst:132
msgid "``junnyu/ckiplab-bert-base-chinese-pos``"
msgstr ""

#: ../model_zoo/transformers.rst:132
msgid "12-layer, 768-hidden, 12-heads, 102M parameters. Finetuned on POS task."
msgstr ""

#: ../model_zoo/transformers.rst:136
msgid "``junnyu/ckiplab-bert-base-chinese-ws``"
msgstr ""

#: ../model_zoo/transformers.rst:136
msgid "12-layer, 768-hidden, 12-heads, 102M parameters. Finetuned on WS task."
msgstr ""

#: ../model_zoo/transformers.rst:140
msgid "``junnyu/nlptown-bert-base-multilingual-uncased-sentiment``"
msgstr ""

#: ../model_zoo/transformers.rst:140
msgid ""
"12-layer, 768-hidden, 12-heads, 167M parameters. Finetuned for sentiment "
"analysis on product reviews in six languages: English, Dutch, German, "
"French, Spanish and Italian."
msgstr ""

#: ../model_zoo/transformers.rst:147
msgid "``junnyu/tbs17-MathBERT``"
msgstr ""

#: ../model_zoo/transformers.rst:147
msgid ""
"12-layer, 768-hidden, 12-heads, 110M parameters. Trained on pre-k to "
"graduate math language (English) using a masked language modeling (MLM) "
"objective."
msgstr ""

#: ../model_zoo/transformers.rst:153
msgid "``macbert-base-chinese``"
msgstr ""

#: ../model_zoo/transformers.rst:153
msgid ""
"12-layer, 768-hidden, 12-heads, 102M parameters. Trained with novel MLM "
"as correction pre-training task."
msgstr ""

#: ../model_zoo/transformers.rst:158
msgid "``macbert-large-chinese``"
msgstr ""

#: ../model_zoo/transformers.rst:158
msgid ""
"24-layer, 1024-hidden, 16-heads, 326M parameters. Trained with novel MLM "
"as correction pre-training task."
msgstr ""

#: ../model_zoo/transformers.rst:163
msgid "``simbert-base-chinese``"
msgstr ""

#: ../model_zoo/transformers.rst:163
msgid ""
"12-layer, 768-hidden, 12-heads, 108M parameters. Trained on 22 million "
"pairs of similar sentences crawed from Baidu Know."
msgstr ""

#: ../model_zoo/transformers.rst:168
msgid "BERT-Japanese_"
msgstr ""

#: ../model_zoo/transformers.rst:168
msgid "``iverxin/bert-base-japanese``"
msgstr ""

#: ../model_zoo/transformers.rst:168 ../model_zoo/transformers.rst:172
#: ../model_zoo/transformers.rst:177 ../model_zoo/transformers.rst:181
msgid "Japanese"
msgstr ""

#: ../model_zoo/transformers.rst:168
msgid "12-layer, 768-hidden, 12-heads, 110M parameters. Trained on Japanese text."
msgstr ""

#: ../model_zoo/transformers.rst:172
msgid "``iverxin/bert-base-japanese-whole-word-masking``"
msgstr ""

#: ../model_zoo/transformers.rst:172
msgid ""
"12-layer, 768-hidden, 12-heads, 109M parameters. Trained on Japanese text"
" using Whole-Word-Masking."
msgstr ""

#: ../model_zoo/transformers.rst:177
msgid "``iverxin/bert-base-japanese-char``"
msgstr ""

#: ../model_zoo/transformers.rst:177
msgid ""
"12-layer, 768-hidden, 12-heads, 89M parameters. Trained on Japanese char "
"text."
msgstr ""

#: ../model_zoo/transformers.rst:181
msgid "``iverxin/bert-base-japanese-char-whole-word-masking``"
msgstr ""

#: ../model_zoo/transformers.rst:181
msgid ""
"12-layer, 768-hidden, 12-heads, 89M parameters. Trained on Japanese char "
"text using Whole-Word-Masking."
msgstr ""

#: ../model_zoo/transformers.rst:186 ../model_zoo/transformers.rst:546
msgid "BigBird_"
msgstr ""

#: ../model_zoo/transformers.rst:186
msgid "``bigbird-base-uncased``"
msgstr ""

#: ../model_zoo/transformers.rst:186
msgid ""
"12-layer, 768-hidden, 12-heads, 127M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers.rst:190 ../model_zoo/transformers.rst:548
msgid "Blenderbot_"
msgstr ""

#: ../model_zoo/transformers.rst:190
msgid "``blenderbot-3B``"
msgstr ""

#: ../model_zoo/transformers.rst:190
msgid "26-layer, 32-heads, 3B parameters. The Blenderbot base model."
msgstr ""

#: ../model_zoo/transformers.rst:194
msgid "``blenderbot-400M-distill``"
msgstr ""

#: ../model_zoo/transformers.rst:194
msgid ""
"14-layer, 384-hidden, 32-heads, 400M parameters. The Blenderbot distil "
"model."
msgstr ""

#: ../model_zoo/transformers.rst:198
msgid "``blenderbot-1B-distill``"
msgstr ""

#: ../model_zoo/transformers.rst:198
msgid "14-layer, 32-heads, 1478M parameters. The Blenderbot Distil 1B model."
msgstr ""

#: ../model_zoo/transformers.rst:202 ../model_zoo/transformers.rst:550
msgid "Blenderbot-Small_"
msgstr ""

#: ../model_zoo/transformers.rst:202
msgid "``blenderbot_small-90M``"
msgstr ""

#: ../model_zoo/transformers.rst:202
msgid "16-layer, 16-heads, 90M parameters. The Blenderbot small model."
msgstr ""

#: ../model_zoo/transformers.rst:206 ../model_zoo/transformers.rst:552
msgid "ConvBert_"
msgstr ""

#: ../model_zoo/transformers.rst:206
msgid "``convbert-base``"
msgstr ""

#: ../model_zoo/transformers.rst:206
msgid "12-layer, 768-hidden, 12-heads, 106M parameters. The ConvBERT base model."
msgstr ""

#: ../model_zoo/transformers.rst:210
msgid "``convbert-medium-small``"
msgstr ""

#: ../model_zoo/transformers.rst:210
msgid ""
"12-layer, 384-hidden, 8-heads, 17M parameters. The ConvBERT medium small "
"model."
msgstr ""

#: ../model_zoo/transformers.rst:214
msgid "``convbert-small``"
msgstr ""

#: ../model_zoo/transformers.rst:214
msgid "12-layer, 128-hidden, 4-heads, 13M parameters. The ConvBERT small model."
msgstr ""

#: ../model_zoo/transformers.rst:218 ../model_zoo/transformers.rst:554
msgid "CTRL_"
msgstr ""

#: ../model_zoo/transformers.rst:218
msgid "``ctrl``"
msgstr ""

#: ../model_zoo/transformers.rst:218
msgid "48-layer, 1280-hidden, 16-heads, 1701M parameters. The CTRL base model."
msgstr ""

#: ../model_zoo/transformers.rst:222
msgid "``sshleifer-tiny-ctrl``"
msgstr ""

#: ../model_zoo/transformers.rst:222
msgid "2-layer, 16-hidden, 2-heads, 5M parameters. The Tiny CTRL model."
msgstr ""

#: ../model_zoo/transformers.rst:226 ../model_zoo/transformers.rst:556
msgid "DistilBert_"
msgstr ""

#: ../model_zoo/transformers.rst:226
msgid "``distilbert-base-uncased``"
msgstr ""

#: ../model_zoo/transformers.rst:226
msgid ""
"6-layer, 768-hidden, 12-heads, 66M parameters. The DistilBERT model "
"distilled from the BERT model ``bert-base-uncased``"
msgstr ""

#: ../model_zoo/transformers.rst:231
msgid "``distilbert-base-cased``"
msgstr ""

#: ../model_zoo/transformers.rst:231
msgid ""
"6-layer, 768-hidden, 12-heads, 66M parameters. The DistilBERT model "
"distilled from the BERT model ``bert-base-cased``"
msgstr ""

#: ../model_zoo/transformers.rst:236 ../model_zoo/transformers.rst:558
msgid "ELECTRA_"
msgstr ""

#: ../model_zoo/transformers.rst:236
msgid "``electra-small``"
msgstr ""

#: ../model_zoo/transformers.rst:236
msgid ""
"12-layer, 768-hidden, 4-heads, _M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers.rst:240
msgid "``electra-base``"
msgstr ""

#: ../model_zoo/transformers.rst:240
msgid ""
"12-layer, 768-hidden, 12-heads, _M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers.rst:244
msgid "``electra-large``"
msgstr ""

#: ../model_zoo/transformers.rst:244
msgid ""
"24-layer, 1024-hidden, 16-heads, _M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers.rst:248
msgid "``chinese-electra-small``"
msgstr ""

#: ../model_zoo/transformers.rst:248
msgid "12-layer, 768-hidden, 4-heads, _M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers.rst:252
msgid "``chinese-electra-base``"
msgstr ""

#: ../model_zoo/transformers.rst:252
msgid "12-layer, 768-hidden, 12-heads, _M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers.rst:256
msgid "``junnyu/hfl-chinese-electra-180g-base-discriminator``"
msgstr ""

#: ../model_zoo/transformers.rst:256
msgid ""
"Discriminator, 12-layer, 768-hidden, 12-heads, 102M parameters. Trained "
"on 180g Chinese text."
msgstr ""

#: ../model_zoo/transformers.rst:260
msgid "``junnyu/hfl-chinese-electra-180g-small-ex-discriminator``"
msgstr ""

#: ../model_zoo/transformers.rst:260
msgid ""
"Discriminator, 24-layer, 256-hidden, 4-heads, 24M parameters. Trained on "
"180g Chinese text."
msgstr ""

#: ../model_zoo/transformers.rst:264
msgid "``junnyu/hfl-chinese-legal-electra-small-generator``"
msgstr ""

#: ../model_zoo/transformers.rst:264
msgid ""
"Generator, 12-layer, 64-hidden, 1-heads, 3M parameters. Trained on "
"Chinese legal corpus."
msgstr ""

#: ../model_zoo/transformers.rst:268 ../model_zoo/transformers.rst:560
msgid "ERNIE_"
msgstr ""

#: ../model_zoo/transformers.rst:268
msgid "``ernie-1.0``"
msgstr ""

#: ../model_zoo/transformers.rst:268 ../model_zoo/transformers.rst:288
#: ../model_zoo/transformers.rst:309 ../model_zoo/transformers.rst:362
#: ../model_zoo/transformers.rst:491
msgid "12-layer, 768-hidden, 12-heads, 108M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers.rst:272
msgid "``ernie-tiny``"
msgstr ""

#: ../model_zoo/transformers.rst:272
msgid "3-layer, 1024-hidden, 16-heads, _M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers.rst:276
msgid "``ernie-2.0-en``"
msgstr ""

#: ../model_zoo/transformers.rst:276 ../model_zoo/transformers.rst:292
msgid ""
"12-layer, 768-hidden, 12-heads, 103M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers.rst:280
msgid "``ernie-2.0-en-finetuned-squad``"
msgstr ""

#: ../model_zoo/transformers.rst:280
msgid ""
"12-layer, 768-hidden, 12-heads, 110M parameters. Trained on finetuned "
"squad text."
msgstr ""

#: ../model_zoo/transformers.rst:284
msgid "``ernie-2.0-large-en``"
msgstr ""

#: ../model_zoo/transformers.rst:288 ../model_zoo/transformers.rst:562
msgid "ERNIE-DOC_"
msgstr ""

#: ../model_zoo/transformers.rst:288
msgid "``ernie-doc-base-zh``"
msgstr ""

#: ../model_zoo/transformers.rst:292
msgid "``ernie-doc-base-en``"
msgstr ""

#: ../model_zoo/transformers.rst:296 ../model_zoo/transformers.rst:564
msgid "ERNIE-GEN_"
msgstr ""

#: ../model_zoo/transformers.rst:296
msgid "``ernie-gen-base-en``"
msgstr ""

#: ../model_zoo/transformers.rst:296
msgid ""
"12-layer, 768-hidden, 12-heads, 108M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers.rst:300
msgid "``ernie-gen-large-en``"
msgstr ""

#: ../model_zoo/transformers.rst:304
msgid "``ernie-gen-large-en-430g``"
msgstr ""

#: ../model_zoo/transformers.rst:304
msgid ""
"24-layer, 1024-hidden, 16-heads, 336M parameters. Trained on lower-cased "
"English text. with extended data (430 GB)."
msgstr ""

#: ../model_zoo/transformers.rst:309 ../model_zoo/transformers.rst:566
msgid "ERNIE-GRAM_"
msgstr ""

#: ../model_zoo/transformers.rst:309
msgid "``ernie-gram-zh``"
msgstr ""

#: ../model_zoo/transformers.rst:313 ../model_zoo/transformers.rst:568
msgid "GPT_"
msgstr ""

#: ../model_zoo/transformers.rst:313
msgid "``gpt-cpm-large-cn``"
msgstr ""

#: ../model_zoo/transformers.rst:313
msgid "32-layer, 2560-hidden, 32-heads, 2.6B parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers.rst:317
msgid "``gpt-cpm-small-cn-distill``"
msgstr ""

#: ../model_zoo/transformers.rst:317
msgid ""
"12-layer, 768-hidden, 12-heads, 109M parameters. The model distilled from"
" the GPT model ``gpt-cpm-large-cn``"
msgstr ""

#: ../model_zoo/transformers.rst:322
msgid "``gpt2-en``"
msgstr ""

#: ../model_zoo/transformers.rst:322
msgid "12-layer, 768-hidden, 12-heads, 117M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers.rst:326
msgid "``gpt2-medium-en``"
msgstr ""

#: ../model_zoo/transformers.rst:326
msgid "24-layer, 1024-hidden, 16-heads, 345M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers.rst:330
msgid "``gpt2-large-en``"
msgstr ""

#: ../model_zoo/transformers.rst:330 ../model_zoo/transformers.rst:350
msgid "36-layer, 1280-hidden, 20-heads, 774M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers.rst:334
msgid "``gpt2-xl-en``"
msgstr ""

#: ../model_zoo/transformers.rst:334
msgid ""
"48-layer, 1600-hidden, 25-heads, 1558M parameters. Trained on English "
"text."
msgstr ""

#: ../model_zoo/transformers.rst:338
msgid "``junnyu/distilgpt2``"
msgstr ""

#: ../model_zoo/transformers.rst:338
msgid "6-layer, 768-hidden, 12-heads, 81M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers.rst:342
msgid "``junnyu/microsoft-DialoGPT-small``"
msgstr ""

#: ../model_zoo/transformers.rst:342
msgid "12-layer, 768-hidden, 12-heads, 124M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers.rst:346
msgid "``junnyu/microsoft-DialoGPT-medium``"
msgstr ""

#: ../model_zoo/transformers.rst:346
msgid "24-layer, 1024-hidden, 16-heads, 354M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers.rst:350
msgid "``junnyu/microsoft-DialoGPT-large``"
msgstr ""

#: ../model_zoo/transformers.rst:354
msgid "``junnyu/uer-gpt2-chinese-poem``"
msgstr ""

#: ../model_zoo/transformers.rst:354
msgid ""
"12-layer, 768-hidden, 12-heads, 103M parameters. Trained on Chinese "
"poetry corpus."
msgstr ""

#: ../model_zoo/transformers.rst:358 ../model_zoo/transformers.rst:570
msgid "MPNet_"
msgstr ""

#: ../model_zoo/transformers.rst:358
msgid "``mpnet-base``"
msgstr ""

#: ../model_zoo/transformers.rst:358
msgid "12-layer, 768-hidden, 12-heads, 109M parameters. MPNet Base Model."
msgstr ""

#: ../model_zoo/transformers.rst:362 ../model_zoo/transformers.rst:572
msgid "NeZha_"
msgstr ""

#: ../model_zoo/transformers.rst:362
msgid "``nezha-base-chinese``"
msgstr ""

#: ../model_zoo/transformers.rst:366
msgid "``nezha-large-chinese``"
msgstr ""

#: ../model_zoo/transformers.rst:366 ../model_zoo/transformers.rst:374
msgid "24-layer, 1024-hidden, 16-heads, 336M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers.rst:370
msgid "``nezha-base-wwm-chinese``"
msgstr ""

#: ../model_zoo/transformers.rst:370
msgid "12-layer, 768-hidden, 16-heads, 108M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers.rst:374
msgid "``nezha-large-wwm-chinese``"
msgstr ""

#: ../model_zoo/transformers.rst:378 ../model_zoo/transformers.rst:574
msgid "RoBERTa_"
msgstr ""

#: ../model_zoo/transformers.rst:378
msgid "``roberta-wwm-ext``"
msgstr ""

#: ../model_zoo/transformers.rst:378
msgid ""
"12-layer, 768-hidden, 12-heads, 102M parameters. Trained on English Text "
"using Whole-Word-Masking with extended data."
msgstr ""

#: ../model_zoo/transformers.rst:383
msgid "``roberta-wwm-ext-large``"
msgstr ""

#: ../model_zoo/transformers.rst:383
msgid ""
"24-layer, 1024-hidden, 16-heads, 325M parameters. Trained on English Text"
" using Whole-Word-Masking with extended data."
msgstr ""

#: ../model_zoo/transformers.rst:388
msgid "``rbt3``"
msgstr ""

#: ../model_zoo/transformers.rst:388
msgid "3-layer, 768-hidden, 12-heads, 38M parameters."
msgstr ""

#: ../model_zoo/transformers.rst:391
msgid "``rbtl3``"
msgstr ""

#: ../model_zoo/transformers.rst:391
msgid "3-layer, 1024-hidden, 16-heads, 61M parameters."
msgstr ""

#: ../model_zoo/transformers.rst:394 ../model_zoo/transformers.rst:576
msgid "RoFormer_"
msgstr ""

#: ../model_zoo/transformers.rst:394
msgid "``roformer-chinese-small``"
msgstr ""

#: ../model_zoo/transformers.rst:394
msgid ""
"6-layer, 384-hidden, 6-heads, 30M parameters. Roformer Small Chinese "
"model."
msgstr ""

#: ../model_zoo/transformers.rst:398
msgid "``roformer-chinese-base``"
msgstr ""

#: ../model_zoo/transformers.rst:398
msgid ""
"12-layer, 768-hidden, 12-heads, 124M parameters. Roformer Base Chinese "
"model."
msgstr ""

#: ../model_zoo/transformers.rst:402
msgid "``roformer-chinese-char-small``"
msgstr ""

#: ../model_zoo/transformers.rst:402
msgid ""
"6-layer, 384-hidden, 6-heads, 15M parameters. Roformer Chinese Char Small"
" model."
msgstr ""

#: ../model_zoo/transformers.rst:406
msgid "``roformer-chinese-char-base``"
msgstr ""

#: ../model_zoo/transformers.rst:406
msgid ""
"12-layer, 768-hidden, 12-heads, 95M parameters. Roformer Chinese Char "
"Base model."
msgstr ""

#: ../model_zoo/transformers.rst:410
msgid "``roformer-chinese-sim-char-ft-small``"
msgstr ""

#: ../model_zoo/transformers.rst:410
msgid ""
"6-layer, 384-hidden, 6-heads, 15M parameters. Roformer Chinese Char Ft "
"Small model."
msgstr ""

#: ../model_zoo/transformers.rst:414
msgid "``roformer-chinese-sim-char-ft-base``"
msgstr ""

#: ../model_zoo/transformers.rst:414
msgid ""
"12-layer, 768-hidden, 12-heads, 95M parameters. Roformer Chinese Char Ft "
"Base model."
msgstr ""

#: ../model_zoo/transformers.rst:418
msgid "``roformer-chinese-sim-char-small``"
msgstr ""

#: ../model_zoo/transformers.rst:418
msgid ""
"6-layer, 384-hidden, 6-heads, 15M parameters. Roformer Chinese Sim Char "
"Small model."
msgstr ""

#: ../model_zoo/transformers.rst:422
msgid "``roformer-chinese-sim-char-base``"
msgstr ""

#: ../model_zoo/transformers.rst:422
msgid ""
"12-layer, 768-hidden, 12-heads, 95M parameters. Roformer Chinese Sim Char"
" Base model."
msgstr ""

#: ../model_zoo/transformers.rst:426
msgid "``roformer-english-small-discriminator``"
msgstr ""

#: ../model_zoo/transformers.rst:426
msgid ""
"12-layer, 256-hidden, 4-heads, 13M parameters. Roformer English Small "
"Discriminator."
msgstr ""

#: ../model_zoo/transformers.rst:430
msgid "``roformer-english-small-generator``"
msgstr ""

#: ../model_zoo/transformers.rst:430
msgid ""
"12-layer, 64-hidden, 1-heads, 5M parameters. Roformer English Small "
"Generator."
msgstr ""

#: ../model_zoo/transformers.rst:434 ../model_zoo/transformers.rst:578
msgid "SKEP_"
msgstr ""

#: ../model_zoo/transformers.rst:434
msgid "``skep_ernie_1.0_large_ch``"
msgstr ""

#: ../model_zoo/transformers.rst:434
msgid ""
"24-layer, 1024-hidden, 16-heads, 336M parameters. Trained using the Erine"
" model ``ernie_1.0``"
msgstr ""

#: ../model_zoo/transformers.rst:439
msgid "``skep_ernie_2.0_large_en``"
msgstr ""

#: ../model_zoo/transformers.rst:439
msgid ""
"24-layer, 1024-hidden, 16-heads, 336M parameters. Trained using the Erine"
" model ``ernie_2.0_large_en``"
msgstr ""

#: ../model_zoo/transformers.rst:444
msgid "``skep_roberta_large_en``"
msgstr ""

#: ../model_zoo/transformers.rst:444
msgid ""
"24-layer, 1024-hidden, 16-heads, 355M parameters. Trained using the "
"RoBERTa model ``roberta_large_en``"
msgstr ""

#: ../model_zoo/transformers.rst:449 ../model_zoo/transformers.rst:580
msgid "SqueezeBert_"
msgstr ""

#: ../model_zoo/transformers.rst:449
msgid "``squeezebert-uncased``"
msgstr ""

#: ../model_zoo/transformers.rst:449
msgid "12-layer, 768-hidden, 12-heads, 51M parameters. SqueezeBert Uncased model."
msgstr ""

#: ../model_zoo/transformers.rst:453
msgid "``squeezebert-mnli``"
msgstr ""

#: ../model_zoo/transformers.rst:453
msgid "12-layer, 768-hidden, 12-heads, 51M parameters. SqueezeBert Mnli model."
msgstr ""

#: ../model_zoo/transformers.rst:457
msgid "``squeezebert-mnli-headless``"
msgstr ""

#: ../model_zoo/transformers.rst:457
msgid ""
"12-layer, 768-hidden, 12-heads, 51M parameters. SqueezeBert Mnli Headless"
" model."
msgstr ""

#: ../model_zoo/transformers.rst:461 ../model_zoo/transformers.rst:582
msgid "TinyBert_"
msgstr ""

#: ../model_zoo/transformers.rst:461
msgid "``tinybert-4l-312d``"
msgstr ""

#: ../model_zoo/transformers.rst:461 ../model_zoo/transformers.rst:471
#: ../model_zoo/transformers.rst:481
msgid ""
"4-layer, 312-hidden, 12-heads, 14.5M parameters. The TinyBert model "
"distilled from the BERT model ``bert-base-uncased``"
msgstr ""

#: ../model_zoo/transformers.rst:466
msgid "``tinybert-6l-768d``"
msgstr ""

#: ../model_zoo/transformers.rst:466 ../model_zoo/transformers.rst:476
#: ../model_zoo/transformers.rst:486
msgid ""
"6-layer, 768-hidden, 12-heads, 67M parameters. The TinyBert model "
"distilled from the BERT model ``bert-base-uncased``"
msgstr ""

#: ../model_zoo/transformers.rst:471
msgid "``tinybert-4l-312d-v2``"
msgstr ""

#: ../model_zoo/transformers.rst:476
msgid "``tinybert-6l-768d-v2``"
msgstr ""

#: ../model_zoo/transformers.rst:481
msgid "``tinybert-4l-312d-zh``"
msgstr ""

#: ../model_zoo/transformers.rst:486
msgid "``tinybert-6l-768d-zh``"
msgstr ""

#: ../model_zoo/transformers.rst:491 ../model_zoo/transformers.rst:584
msgid "UnifiedTransformer_"
msgstr ""

#: ../model_zoo/transformers.rst:491
msgid "``unified_transformer-12L-cn``"
msgstr ""

#: ../model_zoo/transformers.rst:495
msgid "``unified_transformer-12L-cn-luge``"
msgstr ""

#: ../model_zoo/transformers.rst:495
msgid ""
"12-layer, 768-hidden, 12-heads, 108M parameters. Trained on Chinese text "
"(LUGE.ai)."
msgstr ""

#: ../model_zoo/transformers.rst:499
msgid "``plato-mini``"
msgstr ""

#: ../model_zoo/transformers.rst:499
msgid "6-layer, 768-hidden, 12-heads, 66M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers.rst:503
msgid "UNIMO_"
msgstr ""

#: ../model_zoo/transformers.rst:503
msgid "``unimo-text-1.0``"
msgstr ""

#: ../model_zoo/transformers.rst:503
msgid "12-layer, 768-hidden, 12-heads, 99M parameters. UNIMO-text-1.0 model."
msgstr ""

#: ../model_zoo/transformers.rst:507
msgid "``unimo-text-1.0-large``"
msgstr ""

#: ../model_zoo/transformers.rst:507
msgid ""
"24-layer, 768-hidden, 16-heads, 316M parameters. UNIMO-text-1.0 large "
"model."
msgstr ""

#: ../model_zoo/transformers.rst:511 ../model_zoo/transformers.rst:586
msgid "XLNet_"
msgstr ""

#: ../model_zoo/transformers.rst:511
msgid "``xlnet-base-cased``"
msgstr ""

#: ../model_zoo/transformers.rst:511
msgid "12-layer, 768-hidden, 12-heads, 110M parameters. XLNet English model"
msgstr ""

#: ../model_zoo/transformers.rst:515
msgid "``xlnet-large-cased``"
msgstr ""

#: ../model_zoo/transformers.rst:515
msgid ""
"24-layer, 1024-hidden, 16-heads, 340M parameters. XLNet Large English "
"model"
msgstr ""

#: ../model_zoo/transformers.rst:519
msgid "``chinese-xlnet-base``"
msgstr ""

#: ../model_zoo/transformers.rst:519
msgid "12-layer, 768-hidden, 12-heads, 117M parameters. XLNet Chinese model"
msgstr ""

#: ../model_zoo/transformers.rst:523
msgid "``chinese-xlnet-mid``"
msgstr ""

#: ../model_zoo/transformers.rst:523
msgid ""
"24-layer, 768-hidden, 12-heads, 209M parameters. XLNet Medium Chinese "
"model"
msgstr ""

#: ../model_zoo/transformers.rst:527
msgid "``chinese-xlnet-large``"
msgstr ""

#: ../model_zoo/transformers.rst:527
msgid "24-layer, 1024-hidden, 16-heads, _M parameters. XLNet Large Chinese model"
msgstr ""

#: ../model_zoo/transformers.rst:535
msgid "Transformer预训练模型适用任务汇总"
msgstr ""

#: ../model_zoo/transformers.rst:538
msgid "Sequence Classification"
msgstr ""

#: ../model_zoo/transformers.rst:538
msgid "Token Classification"
msgstr ""

#: ../model_zoo/transformers.rst:538
msgid "Question Answering"
msgstr ""

#: ../model_zoo/transformers.rst:538
msgid "Text Generation"
msgstr ""

#: ../model_zoo/transformers.rst:538
msgid "Multiple Choice"
msgstr ""

#: ../model_zoo/transformers.rst:540 ../model_zoo/transformers.rst:542
#: ../model_zoo/transformers.rst:544 ../model_zoo/transformers.rst:546
#: ../model_zoo/transformers.rst:548 ../model_zoo/transformers.rst:550
#: ../model_zoo/transformers.rst:552 ../model_zoo/transformers.rst:554
#: ../model_zoo/transformers.rst:556 ../model_zoo/transformers.rst:558
#: ../model_zoo/transformers.rst:560 ../model_zoo/transformers.rst:562
#: ../model_zoo/transformers.rst:564 ../model_zoo/transformers.rst:566
#: ../model_zoo/transformers.rst:568 ../model_zoo/transformers.rst:570
#: ../model_zoo/transformers.rst:572 ../model_zoo/transformers.rst:574
#: ../model_zoo/transformers.rst:576 ../model_zoo/transformers.rst:578
#: ../model_zoo/transformers.rst:580 ../model_zoo/transformers.rst:582
#: ../model_zoo/transformers.rst:584 ../model_zoo/transformers.rst:586
msgid "✅"
msgstr ""

#: ../model_zoo/transformers.rst:540 ../model_zoo/transformers.rst:542
#: ../model_zoo/transformers.rst:544 ../model_zoo/transformers.rst:546
#: ../model_zoo/transformers.rst:548 ../model_zoo/transformers.rst:550
#: ../model_zoo/transformers.rst:554 ../model_zoo/transformers.rst:556
#: ../model_zoo/transformers.rst:558 ../model_zoo/transformers.rst:560
#: ../model_zoo/transformers.rst:562 ../model_zoo/transformers.rst:564
#: ../model_zoo/transformers.rst:566 ../model_zoo/transformers.rst:568
#: ../model_zoo/transformers.rst:570 ../model_zoo/transformers.rst:572
#: ../model_zoo/transformers.rst:574 ../model_zoo/transformers.rst:576
#: ../model_zoo/transformers.rst:578 ../model_zoo/transformers.rst:580
#: ../model_zoo/transformers.rst:582 ../model_zoo/transformers.rst:584
#: ../model_zoo/transformers.rst:586
msgid "❌"
msgstr ""

#: ../model_zoo/transformers.rst:618
msgid "预训练模型使用方法"
msgstr ""

#: ../model_zoo/transformers.rst:620
msgid ""
"PaddleNLP Transformer API在提丰富预训练模型的同时，也降低了用户的使用门槛。 只需十几行代码"
"，用户即可完成模型加载和下游任务Fine-tuning。"
msgstr ""

#: ../model_zoo/transformers.rst:658
msgid ""
"上面的代码给出使用预训练模型的简要示例，更完整详细的示例代码， 可以参考：`使用预训练模型Fine-tune完成中文文本分类任务 "
"<https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/text_classification/pretrained_models/>`_"
msgstr ""

#: ../model_zoo/transformers.rst:661
msgid "加载数据集：PaddleNLP内置了多种数据集，用户可以一键导入所需的数据集。"
msgstr ""

#: ../model_zoo/transformers.rst:662
msgid ""
"加载预训练模型：PaddleNLP的预训练模型可以很容易地通过 ``from_pretrained()`` 方法加载。 第一个参数是汇总表中对应的"
" ``Pretrained Weight``，可加载对应的预训练权重。 ``BertForSequenceClassification`` 初始化"
" ``__init__`` 所需的其他参数，如 ``num_classes`` 等， 也是通过 ``from_pretrained()`` "
"传入。``Tokenizer`` 使用同样的 ``from_pretrained`` 方法加载。"
msgstr ""

#: ../model_zoo/transformers.rst:666
msgid "通过 ``Dataset`` 的 ``map`` 函数，使用 ``tokenizer`` 将 ``dataset`` 从原始文本处理成模型的输入。"
msgstr ""

#: ../model_zoo/transformers.rst:667
msgid "定义 ``BatchSampler`` 和 ``DataLoader``，shuffle数据、组合Batch。"
msgstr ""

#: ../model_zoo/transformers.rst:668
msgid "定义训练所需的优化器，loss函数等，就可以开始进行模型fine-tune任务。"
msgstr ""

#: ../model_zoo/transformers.rst:672
msgid "Reference"
msgstr ""

#: ../model_zoo/transformers.rst:673
msgid ""
"部分中文预训练模型来自： `brightmart/albert_zh "
"<https://github.com/brightmart/albert_zh>`_, `ymcui/Chinese-BERT-wwm "
"<https://github.com/ymcui/Chinese-BERT-wwm>`_, `huawei-noah/Pretrained-"
"Language-Model/TinyBERT <https://github.com/huawei-noah/Pretrained-"
"Language-Model/tree/master/TinyBERT>`_, `ymcui/Chinese-XLNet "
"<https://github.com/ymcui/Chinese-XLNet>`_, "
"`huggingface/xlnet_chinese_large "
"<https://huggingface.co/clue/xlnet_chinese_large>`_, `Knover/luge-"
"dialogue <https://github.com/PaddlePaddle/Knover/tree/luge-dialogue/luge-"
"dialogue>`_, `huawei-noah/Pretrained-Language-Model/NEZHA-PyTorch/ "
"<https://github.com/huawei-noah/Pretrained-Language-Model/tree/master"
"/NEZHA-PyTorch>`_ `ZhuiyiTechnology/simbert "
"<https://github.com/ZhuiyiTechnology/simbert>`_"
msgstr ""

#: ../model_zoo/transformers.rst:682
msgid ""
"Lan, Zhenzhong, et al. \"Albert: A lite bert for self-supervised learning"
" of language representations.\" arXiv preprint arXiv:1909.11942 (2019)."
msgstr ""

#: ../model_zoo/transformers.rst:683
msgid ""
"Lewis, Mike, et al. \"BART: Denoising Sequence-to-Sequence Pre-training "
"for Natural Language Generation, Translation, and Comprehension.\" arXiv "
"preprint arXiv:1910.13461 (2019)."
msgstr ""

#: ../model_zoo/transformers.rst:684
msgid ""
"Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional "
"transformers for language understanding.\" arXiv preprint "
"arXiv:1810.04805 (2018)."
msgstr ""

#: ../model_zoo/transformers.rst:685
msgid ""
"Zaheer, Manzil, et al. \"Big bird: Transformers for longer sequences.\" "
"arXiv preprint arXiv:2007.14062 (2020)."
msgstr ""

#: ../model_zoo/transformers.rst:686
msgid ""
"Stephon, Emily, et al. \"Blenderbot: Recipes for building an open-domain "
"chatbot.\" arXiv preprint arXiv:2004.13637 (2020)."
msgstr ""

#: ../model_zoo/transformers.rst:687
msgid ""
"Stephon, Emily, et al. \"Blenderbot-Small: Recipes for building an open-"
"domain chatbot.\" arXiv preprint arXiv:2004.13637 (2020)."
msgstr ""

#: ../model_zoo/transformers.rst:688
msgid ""
"Jiang, Zihang, et al. \"ConvBERT: Improving BERT with Span-based Dynamic "
"Convolution.\" arXiv preprint arXiv:2008.02496 (2020)."
msgstr ""

#: ../model_zoo/transformers.rst:689
msgid ""
"Nitish, Bryan, et al. \"CTRL: A Conditional Transformer Language Model "
"for Controllable Generation.\" arXiv preprint arXiv:1909.05858 (2019)."
msgstr ""

#: ../model_zoo/transformers.rst:690
msgid ""
"Sanh, Victor, et al. \"DistilBERT, a distilled version of BERT: smaller, "
"faster, cheaper and lighter.\" arXiv preprint arXiv:1910.01108 (2019)."
msgstr ""

#: ../model_zoo/transformers.rst:691
msgid ""
"Clark, Kevin, et al. \"Electra: Pre-training text encoders as "
"discriminators rather than generators.\" arXiv preprint arXiv:2003.10555 "
"(2020)."
msgstr ""

#: ../model_zoo/transformers.rst:692
msgid ""
"Sun, Yu, et al. \"Ernie: Enhanced representation through knowledge "
"integration.\" arXiv preprint arXiv:1904.09223 (2019)."
msgstr ""

#: ../model_zoo/transformers.rst:693
msgid ""
"Xiao, Dongling, et al. \"Ernie-gen: An enhanced multi-flow pre-training "
"and fine-tuning framework for natural language generation.\" arXiv "
"preprint arXiv:2001.11314 (2020)."
msgstr ""

#: ../model_zoo/transformers.rst:694
msgid ""
"Xiao, Dongling, et al. \"ERNIE-Gram: Pre-Training with Explicitly N-Gram "
"Masked Language Modeling for Natural Language Understanding.\" arXiv "
"preprint arXiv:2010.12148 (2020)."
msgstr ""

#: ../model_zoo/transformers.rst:695
msgid ""
"Radford, Alec, et al. \"Language models are unsupervised multitask "
"learners.\" OpenAI blog 1.8 (2019): 9."
msgstr ""

#: ../model_zoo/transformers.rst:696
msgid ""
"Song, Kaitao, et al. \"MPNet: Masked and Permuted Pre-training for "
"Language Understanding.\" arXiv preprint arXiv:2004.09297 (2020)."
msgstr ""

#: ../model_zoo/transformers.rst:697
msgid ""
"Wei, Junqiu, et al. \"NEZHA: Neural contextualized representation for "
"chinese language understanding.\" arXiv preprint arXiv:1909.00204 (2019)."
msgstr ""

#: ../model_zoo/transformers.rst:698
msgid ""
"Liu, Yinhan, et al. \"Roberta: A robustly optimized bert pretraining "
"approach.\" arXiv preprint arXiv:1907.11692 (2019)."
msgstr ""

#: ../model_zoo/transformers.rst:699
msgid ""
"Su Jianlin, et al. \"RoFormer: Enhanced Transformer with Rotary Position "
"Embedding.\" arXiv preprint arXiv:2104.09864 (2021)."
msgstr ""

#: ../model_zoo/transformers.rst:700
msgid ""
"Tian, Hao, et al. \"SKEP: Sentiment knowledge enhanced pre-training for "
"sentiment analysis.\" arXiv preprint arXiv:2005.05635 (2020)."
msgstr ""

#: ../model_zoo/transformers.rst:701
msgid ""
"Forrest, ALbert, et al. \"SqueezeBERT: What can computer vision teach NLP"
" about efficient neural networks?\" arXiv preprint arXiv:2006.11316 "
"(2020)."
msgstr ""

#: ../model_zoo/transformers.rst:702
msgid ""
"Vaswani, Ashish, et al. \"Attention is all you need.\" arXiv preprint "
"arXiv:1706.03762 (2017)."
msgstr ""

#: ../model_zoo/transformers.rst:703
msgid ""
"Jiao, Xiaoqi, et al. \"Tinybert: Distilling bert for natural language "
"understanding.\" arXiv preprint arXiv:1909.10351 (2019)."
msgstr ""

#: ../model_zoo/transformers.rst:704
msgid ""
"Bao, Siqi, et al. \"Plato-2: Towards building an open-domain chatbot via "
"curriculum learning.\" arXiv preprint arXiv:2006.16779 (2020)."
msgstr ""

#: ../model_zoo/transformers.rst:705
msgid ""
"Yang, Zhilin, et al. \"Xlnet: Generalized autoregressive pretraining for "
"language understanding.\" arXiv preprint arXiv:1906.08237 (2019)."
msgstr ""

#: ../model_zoo/transformers.rst:706
msgid ""
"Cui, Yiming, et al. \"Pre-training with whole word masking for chinese "
"bert.\" arXiv preprint arXiv:1906.08101 (2019)."
msgstr ""

